{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffe32919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/m/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "import json\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf309af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 13:41:28.150726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All TF 2.0 model weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"arpanghoshal/EmoRoBERTa\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"arpanghoshal/EmoRoBERTa\", from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77712786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_float(num):\n",
    "    return np.format_float_positional(num, trim='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6bdad28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences_text = open(\"../data/mask_16.txt\").read()\n",
    "sentences = sent_tokenize(raw_sentences_text)\n",
    "\n",
    "scores = []\n",
    "top_n = 5\n",
    "\n",
    "# detect emotion in sentences\n",
    "with torch.no_grad():\n",
    "    for sentence in sentences:\n",
    "        # sentence string to tokens\n",
    "        tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        # predict emotions\n",
    "        logits = model(**tokens).logits\n",
    "        # logits to probabilities\n",
    "        probabilities = F.softmax(logits, dim = -1).numpy()\n",
    "        # make floats human readable\n",
    "        probabilities = [format_float(p) for p in probabilities[0]]\n",
    "        # get labels for each emotion\n",
    "        labels = [model.config.id2label[i] for i in range(len(probabilities))]\n",
    "        # get top n labels and probabilities\n",
    "        top_n_indexes = np.argpartition(probabilities, -top_n)[-top_n:]\n",
    "        \n",
    "        score = {\"sentence\": sentence,\n",
    "                 # convert filtered arrays to lists for json serialisation\n",
    "                 \"probabilities\": list(np.array(probabilities)[top_n_indexes]),\n",
    "                 \"labels\": list(np.array(labels)[top_n_indexes])}\n",
    "        scores.append(score)\n",
    "        \n",
    "with open('../data/mask16_sentences_emotions.json', 'w') as outfile:\n",
    "    json.dump(scores, outfile, indent=2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7741ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_paragraphs_text = open(\"../data/mask_16_paragraphs.txt\").read()\n",
    "paragraphs = raw_paragraphs_text.split('\\n\\n')\n",
    "\n",
    "# detect emotion in paragraphs\n",
    "\n",
    "scores = []\n",
    "top_n = 10\n",
    "\n",
    "# detect emotion in sentences\n",
    "with torch.no_grad():\n",
    "    for paragraph in paragraphs:\n",
    "        # sentence string to tokens\n",
    "        tokens = tokenizer(paragraph, return_tensors=\"pt\")\n",
    "        # predict emotions\n",
    "        logits = model(**tokens).logits\n",
    "        # logits to probabilities\n",
    "        probabilities = F.softmax(logits, dim = -1).numpy()\n",
    "        # make floats human readable\n",
    "        probabilities = [format_float(p) for p in probabilities[0]]\n",
    "        # get labels for each emotion\n",
    "        labels = [model.config.id2label[i] for i in range(len(probabilities))]\n",
    "        # get top n labels and probabilities\n",
    "        top_n_indexes = np.argpartition(probabilities, -top_n)[-top_n:]\n",
    "        \n",
    "        score = {\"sentence\": paragraph,\n",
    "                 # convert filtered arrays to lists for json serialisation\n",
    "                 \"probabilities\": list(np.array(probabilities)[top_n_indexes]),\n",
    "                 \"labels\": list(np.array(labels)[top_n_indexes])}\n",
    "        scores.append(score)\n",
    "        \n",
    "with open('../data/mask16_paragraphs_emotions.json', 'w') as outfile:\n",
    "    json.dump(scores, outfile, indent=2) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
